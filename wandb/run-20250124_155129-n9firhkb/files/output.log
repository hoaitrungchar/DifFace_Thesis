trainer:
  target: trainer.TrainerPredictedPrior
model:
  target: models.SwinUnet.SwinUnet
  params:
    config: 1
    patch_size: 4
    num_classes: 1
    embed_dim: 96
    depths:
    - 2
    - 2
    - 2
    - 2
    depths_decoder:
    - 1
    - 2
    - 2
    - 2
    num_heads:
    - 3
    - 6
    - 12
    - 24
    window_size: 4
    qkv_bias: true
    in_chans: 3
    qk_scale: null
    drop_rate: 0.0
    drop_path_rate: 0.1
    ape: false
    patch_norm: true
    use_checkpoint: false
data:
  train:
    type: PriorTraining
    params:
      dataset_type: train
      dir_path:
      - /data/FFHQ/Dataset/FFHQ/train
      noise_path1:
      - /data/FFHQ/Dataset/CelebA-HQ/train
      noise_path2:
      - /data/FFHQ/Dataset/ImageNet/train
      transform_type: default
      transform_kwargs:
        mean:
        - 0.0
        - 0.0
        - 0.0
        std:
        - 1.0
        - 1.0
        - 1.0
      transform_noise_type: crop_norm_train
      transform_noise_kwargs:
        mean:
        - 0.0
        - 0.0
        - 0.0
        std:
        - 1.0
        - 1.0
        - 1.0
        img_resize: 256
        crop_size: 256
      need_path: false
      im_exts:
      - png
      - jpg
      - JPEG
      recursive: false
      kernel_gaussian_size: 3
      img_size: 256
      folder_mask_path:
      - /data/FFHQ/Dataset/Mask/train
      type_prior: edgeCanny
      mask_kwargs:
        nvidia_mask_proba: 1
        nvidia_mask_kwargs:
          folder_mask_path:
          - /data/FFHQ/Dataset/Mask/train
  val:
    type: PriorTraining
    params:
      dataset_type: val
      dir_path:
      - /data/FFHQ/Dataset/FFHQ/val
      noise_path1:
      - /data/FFHQ/Dataset/CelebA-HQ/val
      noise_path2:
      - /data/FFHQ/Dataset/ImageNet/val
      transform_type: default
      transform_kwargs:
        mean:
        - 0.0
        - 0.0
        - 0.0
        std:
        - 1.0
        - 1.0
        - 1.0
      transform_noise_type: crop_norm_val_test
      transform_noise_kwargs:
        mean:
        - 0.0
        - 0.0
        - 0.0
        std:
        - 1.0
        - 1.0
        - 1.0
        img_resize: 256
        crop_size: 256
      need_path: false
      im_exts:
      - png
      - jpg
      - JPEG
      recursive: false
      kernel_gaussian_size: 3
      img_size: 256
      folder_mask_path:
      - /data/FFHQ/Dataset/Mask/val
      type_prior: edgeCanny
      mask_kwargs:
        nvidia_mask_proba: 1
        nvidia_mask_kwargs:
          folder_mask_path:
          - /data/FFHQ/Dataset/Mask/val
train:
  lr: 0.001
  lr_min: 1.0e-06
  batch:
  - 32
  - 32
  microbatch: 32
  num_workers: 8
  prefetch_factor: 2
  iterations: 600000
  weight_decay: 0
  save_freq: 10
  val_freq: ${train.save_freq}
  log_freq:
  - 2
  - 2
  - 3
  ema_rate: 0.999
  loss_type: WCE
  tf_logging: true
  local_logging: true
project_name: Thesis_blind_image_inpainting
group_name: FFHQ_prior
name: test
save_dir: /data/FFHQ/edge_log
resume: ''
cfg_path: /data/FFHQ/DifFace_Thesis/configs/training/predicted_edge_SwinUnet.yaml
seed: 10000

trainer:
  target: trainer.TrainerPredictedPrior
model:
  target: models.SwinUnet.SwinUnet
  params:
    config: 1
    patch_size: 4
    num_classes: 1
    embed_dim: 96
    depths:
    - 2
    - 2
    - 2
    - 2
    depths_decoder:
    - 1
    - 2
    - 2
    - 2
    num_heads:
    - 3
    - 6
    - 12
    - 24
    window_size: 4
    qkv_bias: true
    in_chans: 3
    qk_scale: null
    drop_rate: 0.0
    drop_path_rate: 0.1
    ape: false
    patch_norm: true
    use_checkpoint: false
data:
  train:
    type: PriorTraining
    params:
      dataset_type: train
      dir_path:
      - /data/FFHQ/Dataset/FFHQ/train
      noise_path1:
      - /data/FFHQ/Dataset/CelebA-HQ/train
      noise_path2:
      - /data/FFHQ/Dataset/ImageNet/train
      transform_type: default
      transform_kwargs:
        mean:
        - 0.0
        - 0.0
        - 0.0
        std:
        - 1.0
        - 1.0
        - 1.0
      transform_noise_type: crop_norm_train
      transform_noise_kwargs:
        mean:
        - 0.0
        - 0.0
        - 0.0
        std:
        - 1.0
        - 1.0
        - 1.0
        img_resize: 256
        crop_size: 256
      need_path: false
      im_exts:
      - png
      - jpg
      - JPEG
      recursive: false
      kernel_gaussian_size: 3
      img_size: 256
      folder_mask_path:
      - /data/FFHQ/Dataset/Mask/train
      type_prior: edgeCanny
      mask_kwargs:
        nvidia_mask_proba: 1
        nvidia_mask_kwargs:
          folder_mask_path:
          - /data/FFHQ/Dataset/Mask/train
  val:
    type: PriorTraining
    params:
      dataset_type: val
      dir_path:
      - /data/FFHQ/Dataset/FFHQ/val
      noise_path1:
      - /data/FFHQ/Dataset/CelebA-HQ/val
      noise_path2:
      - /data/FFHQ/Dataset/ImageNet/val
      transform_type: default
      transform_kwargs:
        mean:
        - 0.0
        - 0.0
        - 0.0
        std:
        - 1.0
        - 1.0
        - 1.0
      transform_noise_type: crop_norm_val_test
      transform_noise_kwargs:
        mean:
        - 0.0
        - 0.0
        - 0.0
        std:
        - 1.0
        - 1.0
        - 1.0
        img_resize: 256
        crop_size: 256
      need_path: false
      im_exts:
      - png
      - jpg
      - JPEG
      recursive: false
      kernel_gaussian_size: 3
      img_size: 256
      folder_mask_path:
      - /data/FFHQ/Dataset/Mask/val
      type_prior: edgeCanny
      mask_kwargs:
        nvidia_mask_proba: 1
        nvidia_mask_kwargs:
          folder_mask_path:
          - /data/FFHQ/Dataset/Mask/val
train:
  lr: 0.001
  lr_min: 1.0e-06
  batch:
  - 32
  - 32
  microbatch: 32
  num_workers: 8
  prefetch_factor: 2
  iterations: 600000
  weight_decay: 0
  save_freq: 10
  val_freq: ${train.save_freq}
  log_freq:
  - 2
  - 2
  - 3
  ema_rate: 0.999
  loss_type: WCE
  tf_logging: true
  local_logging: true
project_name: Thesis_blind_image_inpainting
group_name: FFHQ_prior
name: test
save_dir: /data/FFHQ/edge_log
resume: ''
cfg_path: /data/FFHQ/DifFace_Thesis/configs/training/predicted_edge_SwinUnet.yaml
seed: 10000
/home/mmhk20/.local/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)

SwinTransformerSys expand initial----depths:[2, 2, 2, 2];depths_decoder:[1, 2, 2, 2];drop_path_rate:0.1;num_classes:1
/usr/local/anaconda3/lib/python3.8/site-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3587.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
---final upsample expand_first---
Number of parameters: 27.15M
len_file_path_all 54999
24183
1281167
len_file_paths_noise 1305350
len_file_path_all 5000
2993
50000
len_file_paths_noise 52993
Number of images in train data set: 54999
Number of images in val data set: 5000
Train:000002/600000, Loss:4.30e-01, lr:1.00e-03, gradient:1.770341e+00
Train:000004/600000, Loss:2.80e-01, lr:1.00e-03, gradient:3.695998e-01
Train:000006/600000, Loss:2.50e-01, lr:1.00e-03, gradient:2.026664e-01
Train:000008/600000, Loss:2.55e-01, lr:1.00e-03, gradient:2.670751e-01
Train:000010/600000, Loss:2.48e-01, lr:1.00e-03, gradient:1.002938e-01
Elapsed time: 31.00s
============================================================
157 157
val:003/157, loss=0.52322064
val:006/157, loss=0.50817241
val:009/157, loss=0.50894980
val:012/157, loss=0.52284201
val:015/157, loss=0.50402986
val:018/157, loss=0.49569467
val:021/157, loss=0.49715471
val:024/157, loss=0.49798759
val:027/157, loss=0.52410424
val:030/157, loss=0.50666012
val:033/157, loss=0.50688646
val:036/157, loss=0.51082524
val:039/157, loss=0.50409927
val:042/157, loss=0.51249860
val:045/157, loss=0.51107708
val:048/157, loss=0.51386062
val:051/157, loss=0.52322423
val:054/157, loss=0.50702150
val:057/157, loss=0.49852818
val:060/157, loss=0.52011909
val:063/157, loss=0.51586682
val:066/157, loss=0.50858768
val:069/157, loss=0.50891707
val:072/157, loss=0.51840689
val:075/157, loss=0.50724353
val:078/157, loss=0.52077673
val:081/157, loss=0.51735606
val:084/157, loss=0.50208801
val:087/157, loss=0.50554728
val:090/157, loss=0.50152101
val:093/157, loss=0.51870845
val:096/157, loss=0.50528367
val:099/157, loss=0.51006583
val:102/157, loss=0.49859748
val:105/157, loss=0.52304856
val:108/157, loss=0.50271734
val:111/157, loss=0.49292974
val:114/157, loss=0.52389423
val:117/157, loss=0.50737363
val:120/157, loss=0.48436512
val:123/157, loss=0.49618456
val:126/157, loss=0.51411810
val:129/157, loss=0.51476359
val:132/157, loss=0.50315918
val:135/157, loss=0.49914487
val:138/157, loss=0.51199679
val:141/157, loss=0.49732101
val:144/157, loss=0.50220052
val:147/157, loss=0.50840898
val:150/157, loss=0.51661744
val:153/157, loss=0.51966218
val:156/157, loss=0.50261692
loss=0.50884857
Train:000012/600000, Loss:2.74e-01, lr:1.00e-03, gradient:1.807025e-01
Train:000014/600000, Loss:2.53e-01, lr:1.00e-03, gradient:9.455374e-02
Train:000016/600000, Loss:2.38e-01, lr:1.00e-03, gradient:1.274430e-01
Train:000018/600000, Loss:2.47e-01, lr:1.00e-03, gradient:6.165213e-02
Train:000020/600000, Loss:2.54e-01, lr:1.00e-03, gradient:7.356002e-02
Elapsed time: 28.06s
============================================================
157 157
val:003/157, loss=0.45968896
val:006/157, loss=0.44576770
val:009/157, loss=0.44622054
val:012/157, loss=0.46045251
val:015/157, loss=0.44260381
val:018/157, loss=0.43492685
val:021/157, loss=0.43524042
val:024/157, loss=0.43640339
val:027/157, loss=0.46073727
val:030/157, loss=0.44509071
val:033/157, loss=0.44340282
val:036/157, loss=0.44861985
val:039/157, loss=0.44205209
val:042/157, loss=0.45122692
val:045/157, loss=0.44856810
val:048/157, loss=0.45327560
val:051/157, loss=0.45982386
val:054/157, loss=0.44374419
val:057/157, loss=0.43720046
val:060/157, loss=0.45713443
val:063/157, loss=0.45443806
val:066/157, loss=0.44532924
val:069/157, loss=0.44634711
val:072/157, loss=0.45860896
val:075/157, loss=0.44621689
val:078/157, loss=0.45791637
val:081/157, loss=0.45616276
val:084/157, loss=0.44126441
val:087/157, loss=0.44444842
val:090/157, loss=0.44022295
val:093/157, loss=0.45637269
val:096/157, loss=0.44292925
val:099/157, loss=0.44757586
val:102/157, loss=0.43675870
val:105/157, loss=0.46094705
val:108/157, loss=0.44030363
val:111/157, loss=0.43203911
val:114/157, loss=0.45960510
val:117/157, loss=0.44589764
val:120/157, loss=0.42484304
val:123/157, loss=0.43354567
val:126/157, loss=0.45226409
val:129/157, loss=0.45330159
val:132/157, loss=0.44242042
val:135/157, loss=0.43647078
val:138/157, loss=0.45003818
val:141/157, loss=0.43565331
val:144/157, loss=0.44009706
val:147/157, loss=0.44716474
val:150/157, loss=0.45373107
val:153/157, loss=0.45800999
val:156/157, loss=0.44123544
loss=0.44689014
Train:000022/600000, Loss:2.58e-01, lr:1.00e-03, gradient:8.763507e-02
Train:000024/600000, Loss:2.60e-01, lr:1.00e-03, gradient:6.413768e-02
Train:000026/600000, Loss:2.64e-01, lr:1.00e-03, gradient:2.583921e-02
Train:000028/600000, Loss:2.60e-01, lr:1.00e-03, gradient:7.825290e-02
Train:000030/600000, Loss:2.45e-01, lr:1.00e-03, gradient:8.058655e-02
Elapsed time: 28.72s
============================================================
157 157
val:003/157, loss=0.41923024
val:006/157, loss=0.40520798
val:009/157, loss=0.40570689
val:012/157, loss=0.41878552
val:015/157, loss=0.40184343
val:018/157, loss=0.39340715
val:021/157, loss=0.39487240
val:024/157, loss=0.39552742
val:027/157, loss=0.42020584
val:030/157, loss=0.40428128
val:033/157, loss=0.40282670
val:036/157, loss=0.40758788
val:039/157, loss=0.40099750
val:042/157, loss=0.40921919
val:045/157, loss=0.40725296
val:048/157, loss=0.41099005
val:051/157, loss=0.41859534
val:054/157, loss=0.40461005
val:057/157, loss=0.39670886
val:060/157, loss=0.41573554
val:063/157, loss=0.41217641
val:066/157, loss=0.40558083
val:069/157, loss=0.40619438
val:072/157, loss=0.41506078
val:075/157, loss=0.40402215
val:078/157, loss=0.41686056
val:081/157, loss=0.41341397
val:084/157, loss=0.39913935
val:087/157, loss=0.40318722
val:090/157, loss=0.39923018
val:093/157, loss=0.41484119
val:096/157, loss=0.40258895
val:099/157, loss=0.40695476
val:102/157, loss=0.39681400
val:105/157, loss=0.41853013
val:108/157, loss=0.40003165
val:111/157, loss=0.39128348
val:114/157, loss=0.41875462
val:117/157, loss=0.40401448
val:120/157, loss=0.38378814
val:123/157, loss=0.39358766
val:126/157, loss=0.41244976
val:129/157, loss=0.41103290
val:132/157, loss=0.40128479
val:135/157, loss=0.39633684
val:138/157, loss=0.40858908
val:141/157, loss=0.39431935
val:144/157, loss=0.39982012
val:147/157, loss=0.40477969
val:150/157, loss=0.41250405
val:153/157, loss=0.41575753
val:156/157, loss=0.39922847
loss=0.40577249
Train:000032/600000, Loss:2.53e-01, lr:1.00e-03, gradient:3.905878e-02
Train:000034/600000, Loss:2.49e-01, lr:1.00e-03, gradient:5.308791e-02
Train:000036/600000, Loss:2.47e-01, lr:1.00e-03, gradient:5.518700e-02
Train:000038/600000, Loss:2.42e-01, lr:1.00e-03, gradient:2.246542e-02
Train:000040/600000, Loss:2.44e-01, lr:1.00e-03, gradient:2.359679e-02
Elapsed time: 27.65s
============================================================
157 157
val:003/157, loss=0.38693536
Traceback (most recent call last):
  File "main.py", line 32, in <module>
    trainer.train()
  File "/data/FFHQ/DifFace_Thesis/trainer.py", line 323, in train
    self.validation()
  File "/data/FFHQ/DifFace_Thesis/trainer.py", line 1284, in validation
    for ii, data in enumerate(self.dataloaders[phase]):
  File "/usr/local/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/usr/local/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/usr/local/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/data/FFHQ/DifFace_Thesis/datapipe/datasets.py", line 348, in __getitem__
    im_masked = im *  (mask_reshape) + (1-mask_reshape)*noise
KeyboardInterrupt
Traceback (most recent call last):
  File "main.py", line 32, in <module>
    trainer.train()
  File "/data/FFHQ/DifFace_Thesis/trainer.py", line 323, in train
    self.validation()
  File "/data/FFHQ/DifFace_Thesis/trainer.py", line 1284, in validation
    for ii, data in enumerate(self.dataloaders[phase]):
  File "/usr/local/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/usr/local/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/usr/local/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/data/FFHQ/DifFace_Thesis/datapipe/datasets.py", line 348, in __getitem__
    im_masked = im *  (mask_reshape) + (1-mask_reshape)*noise
KeyboardInterrupt
