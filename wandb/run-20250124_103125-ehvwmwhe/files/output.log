trainer:
  target: trainer.TrainerPredictedPrior
model:
  target: models.SwinUnet.SwinUnet
  params:
    config: 1
    patch_size: 4
    num_classes: 1
    embed_dim: 96
    depths:
    - 2
    - 2
    - 2
    - 2
    depths_decoder:
    - 1
    - 2
    - 2
    - 2
    num_heads:
    - 3
    - 6
    - 12
    - 24
    window_size: 4
    qkv_bias: true
    in_chans: 3
    qk_scale: null
    drop_rate: 0.0
    drop_path_rate: 0.1
    ape: false
    patch_norm: true
    use_checkpoint: false
data:
  train:
    type: PriorTraining
    params:
      dataset_type: train
      dir_path:
      - /data/FFHQ/Dataset/FFHQ/train
      noise_path1:
      - /data/FFHQ/Dataset/CelebA-HQ/train
      noise_path2:
      - /data/FFHQ/Dataset/ImageNet/train
      transform_type: default
      transform_kwargs:
        mean:
        - 0.0
        - 0.0
        - 0.0
        std:
        - 1.0
        - 1.0
        - 1.0
      transform_noise_type: crop_norm_train
      transform_noise_kwargs:
        mean:
        - 0.0
        - 0.0
        - 0.0
        std:
        - 1.0
        - 1.0
        - 1.0
        img_resize: 256
        crop_size: 256
      need_path: false
      im_exts:
      - png
      - jpg
      - JPEG
      recursive: false
      kernel_gaussian_size: 3
      img_size: 256
      folder_mask_path:
      - /data/FFHQ/Dataset/Mask/train
      type_prior: edgeCanny
      mask_kwargs:
        nvidia_mask_proba: 1
        nvidia_mask_kwargs:
          folder_mask_path:
          - /data/FFHQ/Dataset/Mask/train
  val:
    type: PriorTraining
    params:
      dataset_type: val
      dir_path:
      - /data/FFHQ/Dataset/FFHQ/val
      noise_path1:
      - /data/FFHQ/Dataset/CelebA-HQ/val
      noise_path2:
      - /data/FFHQ/Dataset/ImageNet/val
      transform_type: default
      transform_kwargs:
        mean:
        - 0.0
        - 0.0
        - 0.0
        std:
        - 1.0
        - 1.0
        - 1.0
      transform_noise_type: crop_norm_val_test
      transform_noise_kwargs:
        mean:
        - 0.0
        - 0.0
        - 0.0
        std:
        - 1.0
        - 1.0
        - 1.0
        img_resize: 256
        crop_size: 256
      need_path: false
      im_exts:
      - png
      - jpg
      - JPEG
      recursive: false
      kernel_gaussian_size: 3
      img_size: 256
      folder_mask_path:
      - /data/FFHQ/Dataset/Mask/val
      type_prior: edgeCanny
      mask_kwargs:
        nvidia_mask_proba: 1
        nvidia_mask_kwargs:
          folder_mask_path:
          - /data/FFHQ/Dataset/Mask/val
train:
  lr: 0.001
  lr_min: 1.0e-06
  batch:
  - 32
  - 32
  microbatch: 32
  num_workers: 8
  prefetch_factor: 2
  iterations: 600000
  weight_decay: 0
  save_freq: 10
  val_freq: ${train.save_freq}
  log_freq:
  - 2
  - 2
  - 3
  ema_rate: 0.999
  loss_type: WCE
  tf_logging: true
  local_logging: true
project_name: Thesis_blind_image_inpainting
group_name: test
name: test
save_dir: /data/FFHQ/edge_log
resume: ''
cfg_path: /data/FFHQ/DifFace_Thesis/configs/training/predicted_edge_SwinUnet.yaml
seed: 10000

trainer:
  target: trainer.TrainerPredictedPrior
model:
  target: models.SwinUnet.SwinUnet
  params:
    config: 1
    patch_size: 4
    num_classes: 1
    embed_dim: 96
    depths:
    - 2
    - 2
    - 2
    - 2
    depths_decoder:
    - 1
    - 2
    - 2
    - 2
    num_heads:
    - 3
    - 6
    - 12
    - 24
    window_size: 4
    qkv_bias: true
    in_chans: 3
    qk_scale: null
    drop_rate: 0.0
    drop_path_rate: 0.1
    ape: false
    patch_norm: true
    use_checkpoint: false
data:
  train:
    type: PriorTraining
    params:
      dataset_type: train
      dir_path:
      - /data/FFHQ/Dataset/FFHQ/train
      noise_path1:
      - /data/FFHQ/Dataset/CelebA-HQ/train
      noise_path2:
      - /data/FFHQ/Dataset/ImageNet/train
      transform_type: default
      transform_kwargs:
        mean:
        - 0.0
        - 0.0
        - 0.0
        std:
        - 1.0
        - 1.0
        - 1.0
      transform_noise_type: crop_norm_train
      transform_noise_kwargs:
        mean:
        - 0.0
        - 0.0
        - 0.0
        std:
        - 1.0
        - 1.0
        - 1.0
        img_resize: 256
        crop_size: 256
      need_path: false
      im_exts:
      - png
      - jpg
      - JPEG
      recursive: false
      kernel_gaussian_size: 3
      img_size: 256
      folder_mask_path:
      - /data/FFHQ/Dataset/Mask/train
      type_prior: edgeCanny
      mask_kwargs:
        nvidia_mask_proba: 1
        nvidia_mask_kwargs:
          folder_mask_path:
          - /data/FFHQ/Dataset/Mask/train
  val:
    type: PriorTraining
    params:
      dataset_type: val
      dir_path:
      - /data/FFHQ/Dataset/FFHQ/val
      noise_path1:
      - /data/FFHQ/Dataset/CelebA-HQ/val
      noise_path2:
      - /data/FFHQ/Dataset/ImageNet/val
      transform_type: default
      transform_kwargs:
        mean:
        - 0.0
        - 0.0
        - 0.0
        std:
        - 1.0
        - 1.0
        - 1.0
      transform_noise_type: crop_norm_val_test
      transform_noise_kwargs:
        mean:
        - 0.0
        - 0.0
        - 0.0
        std:
        - 1.0
        - 1.0
        - 1.0
        img_resize: 256
        crop_size: 256
      need_path: false
      im_exts:
      - png
      - jpg
      - JPEG
      recursive: false
      kernel_gaussian_size: 3
      img_size: 256
      folder_mask_path:
      - /data/FFHQ/Dataset/Mask/val
      type_prior: edgeCanny
      mask_kwargs:
        nvidia_mask_proba: 1
        nvidia_mask_kwargs:
          folder_mask_path:
          - /data/FFHQ/Dataset/Mask/val
train:
  lr: 0.001
  lr_min: 1.0e-06
  batch:
  - 32
  - 32
  microbatch: 32
  num_workers: 8
  prefetch_factor: 2
  iterations: 600000
  weight_decay: 0
  save_freq: 10
  val_freq: ${train.save_freq}
  log_freq:
  - 2
  - 2
  - 3
  ema_rate: 0.999
  loss_type: WCE
  tf_logging: true
  local_logging: true
project_name: Thesis_blind_image_inpainting
group_name: test
name: test
save_dir: /data/FFHQ/edge_log
resume: ''
cfg_path: /data/FFHQ/DifFace_Thesis/configs/training/predicted_edge_SwinUnet.yaml
seed: 10000
/home/mmhk20/.local/lib/python3.8/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)

SwinTransformerSys expand initial----depths:[2, 2, 2, 2];depths_decoder:[1, 2, 2, 2];drop_path_rate:0.1;num_classes:1
/usr/local/anaconda3/lib/python3.8/site-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3587.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
---final upsample expand_first---
Number of parameters: 27.15M
len_file_path_all 54999
24183
1281167
len_file_paths_noise 1305350
len_file_path_all 5000
2993
50000
len_file_paths_noise 52993
Number of images in train data set: 54999
Number of images in val data set: 5000
Train:000002/600000, Loss:4.30e-01, lr:1.00e-03, gradient:1.770341e+00
Train:000004/600000, Loss:2.80e-01, lr:1.00e-03, gradient:3.695998e-01
Train:000006/600000, Loss:2.50e-01, lr:1.00e-03, gradient:2.026664e-01
Train:000008/600000, Loss:2.55e-01, lr:1.00e-03, gradient:2.670751e-01
Train:000010/600000, Loss:2.48e-01, lr:1.00e-03, gradient:1.002938e-01
Elapsed time: 25.33s
============================================================
val:003/157, loss=0.51747805
val:006/157, loss=0.51108247
val:009/157, loss=0.52542078
val:012/157, loss=0.51087171
val:015/157, loss=0.48948774
val:018/157, loss=0.50509208
val:021/157, loss=0.49236837
val:024/157, loss=0.51822078
val:027/157, loss=0.54205477
val:030/157, loss=0.50740522
val:033/157, loss=0.51132309
val:036/157, loss=0.51209927
val:039/157, loss=0.50261748
val:042/157, loss=0.51192009
val:045/157, loss=0.48157153
val:048/157, loss=0.51469398
val:051/157, loss=0.50043076
val:054/157, loss=0.49092776
val:057/157, loss=0.51940346
val:060/157, loss=0.53686798
val:063/157, loss=0.51988649
val:066/157, loss=0.51073062
val:069/157, loss=0.50269508
val:072/157, loss=0.52469712
val:075/157, loss=0.48104054
val:078/157, loss=0.52124888
val:081/157, loss=0.51472545
val:084/157, loss=0.51431143
val:087/157, loss=0.52260417
val:090/157, loss=0.51462573
val:093/157, loss=0.51262152
val:096/157, loss=0.50055432
val:099/157, loss=0.52195370
val:102/157, loss=0.51477194
val:105/157, loss=0.55195701
val:108/157, loss=0.49989772
val:111/157, loss=0.48321345
val:114/157, loss=0.53768349
val:117/157, loss=0.51723540
val:120/157, loss=0.48785970
val:123/157, loss=0.49326378
val:126/157, loss=0.51980066
val:129/157, loss=0.52389032
val:132/157, loss=0.49767372
val:135/157, loss=0.51811999
val:138/157, loss=0.53574395
val:141/157, loss=0.51610386
val:144/157, loss=0.50315177
val:147/157, loss=0.49591714
val:150/157, loss=0.51368231
val:153/157, loss=0.52268660
val:156/157, loss=0.51481736
loss=1.52654576
Train:000012/600000, Loss:2.74e-01, lr:1.00e-03, gradient:1.807025e-01
Train:000014/600000, Loss:2.53e-01, lr:1.00e-03, gradient:9.455374e-02
Train:000016/600000, Loss:2.38e-01, lr:1.00e-03, gradient:1.274430e-01
Train:000018/600000, Loss:2.47e-01, lr:1.00e-03, gradient:6.165213e-02
Train:000020/600000, Loss:2.54e-01, lr:1.00e-03, gradient:7.356002e-02
Elapsed time: 21.42s
============================================================
val:003/157, loss=0.45545834
val:006/157, loss=0.44801867
val:009/157, loss=0.46141729
val:012/157, loss=0.44842413
val:015/157, loss=0.42869806
val:018/157, loss=0.44307482
val:021/157, loss=0.43076351
val:024/157, loss=0.45512414
val:027/157, loss=0.47864074
val:030/157, loss=0.44566941
val:033/157, loss=0.44842640
val:036/157, loss=0.44779056
val:039/157, loss=0.44057852
val:042/157, loss=0.44911563
val:045/157, loss=0.42155266
val:048/157, loss=0.45421326
val:051/157, loss=0.43978205
val:054/157, loss=0.42993730
val:057/157, loss=0.45645660
val:060/157, loss=0.47511190
val:063/157, loss=0.45754540
val:066/157, loss=0.44641519
val:069/157, loss=0.43937474
val:072/157, loss=0.46488994
val:075/157, loss=0.42294255
val:078/157, loss=0.45666367
val:081/157, loss=0.45277297
val:084/157, loss=0.45224750
val:087/157, loss=0.45858958
val:090/157, loss=0.45312876
val:093/157, loss=0.44919342
val:096/157, loss=0.43603629
val:099/157, loss=0.45819286
val:102/157, loss=0.45163196
val:105/157, loss=0.48707628
val:108/157, loss=0.43572766
val:111/157, loss=0.42299142
val:114/157, loss=0.47319621
val:117/157, loss=0.45506755
val:120/157, loss=0.42663443
val:123/157, loss=0.43156120
val:126/157, loss=0.45683438
val:129/157, loss=0.46179014
val:132/157, loss=0.43785578
val:135/157, loss=0.45472518
val:138/157, loss=0.47211593
val:141/157, loss=0.45284563
val:144/157, loss=0.44033751
val:147/157, loss=0.43622696
val:150/157, loss=0.45062578
val:153/157, loss=0.46061715
val:156/157, loss=0.45218375
loss=1.34067023
Train:000022/600000, Loss:2.58e-01, lr:1.00e-03, gradient:8.763508e-02
Train:000024/600000, Loss:2.60e-01, lr:1.00e-03, gradient:6.413768e-02
Train:000026/600000, Loss:2.64e-01, lr:1.00e-03, gradient:2.583921e-02
Train:000028/600000, Loss:2.60e-01, lr:1.00e-03, gradient:7.825290e-02
Train:000030/600000, Loss:2.45e-01, lr:1.00e-03, gradient:8.058655e-02
Elapsed time: 22.81s
============================================================
val:003/157, loss=0.41420460
val:006/157, loss=0.40744352
val:009/157, loss=0.42039257
val:012/157, loss=0.40719348
val:015/157, loss=0.38885936
val:018/157, loss=0.40260142
val:021/157, loss=0.39253080
val:024/157, loss=0.41412663
val:027/157, loss=0.43633676
val:030/157, loss=0.40377504
val:033/157, loss=0.40624991
val:036/157, loss=0.40959942
val:039/157, loss=0.39832968
val:042/157, loss=0.40830237
val:045/157, loss=0.38243711
val:048/157, loss=0.41106692
val:051/157, loss=0.39829099
val:054/157, loss=0.39020064
val:057/157, loss=0.41587439
val:060/157, loss=0.43267447
val:063/157, loss=0.41601294
val:066/157, loss=0.40780190
val:069/157, loss=0.40058288
val:072/157, loss=0.42132896
val:075/157, loss=0.38021770
val:078/157, loss=0.41541678
val:081/157, loss=0.41046000
val:084/157, loss=0.41129845
val:087/157, loss=0.41719085
val:090/157, loss=0.41157675
val:093/157, loss=0.40744933
val:096/157, loss=0.39683330
val:099/157, loss=0.41685215
val:102/157, loss=0.40987140
val:105/157, loss=0.44399601
val:108/157, loss=0.39545998
val:111/157, loss=0.38241175
val:114/157, loss=0.43318498
val:117/157, loss=0.41349396
val:120/157, loss=0.38688833
val:123/157, loss=0.39084417
val:126/157, loss=0.41874078
val:129/157, loss=0.41949159
val:132/157, loss=0.39634866
val:135/157, loss=0.41490737
val:138/157, loss=0.43000579
val:141/157, loss=0.41079426
val:144/157, loss=0.40205085
val:147/157, loss=0.39467791
val:150/157, loss=0.40875736
val:153/157, loss=0.42041773
val:156/157, loss=0.40923208
loss=1.21731722
Train:000032/600000, Loss:2.53e-01, lr:1.00e-03, gradient:3.905878e-02
Train:000034/600000, Loss:2.49e-01, lr:1.00e-03, gradient:5.308791e-02
Train:000036/600000, Loss:2.47e-01, lr:1.00e-03, gradient:5.518700e-02
Train:000038/600000, Loss:2.42e-01, lr:1.00e-03, gradient:2.246542e-02
Train:000040/600000, Loss:2.44e-01, lr:1.00e-03, gradient:2.359679e-02
Elapsed time: 22.18s
============================================================
val:003/157, loss=0.38090530
val:006/157, loss=0.37545213
val:009/157, loss=0.38761333
val:012/157, loss=0.37631878
val:015/157, loss=0.35718250
val:018/157, loss=0.37027344
val:021/157, loss=0.36027443
val:024/157, loss=0.38351879
val:027/157, loss=0.40682739
val:030/157, loss=0.37542444
val:033/157, loss=0.37607640
val:036/157, loss=0.37606281
val:039/157, loss=0.36879298
val:042/157, loss=0.37748826
val:045/157, loss=0.35073876
val:048/157, loss=0.37944692
val:051/157, loss=0.36687660
val:054/157, loss=0.36180967
val:057/157, loss=0.38415557
val:060/157, loss=0.40039843
val:063/157, loss=0.38546026
val:066/157, loss=0.37458432
val:069/157, loss=0.36914802
val:072/157, loss=0.39055431
val:075/157, loss=0.35126108
val:078/157, loss=0.38507167
val:081/157, loss=0.37830359
val:084/157, loss=0.38200909
val:087/157, loss=0.38601977
val:090/157, loss=0.37965235
val:093/157, loss=0.37736660
val:096/157, loss=0.36790290
val:099/157, loss=0.38545266
val:102/157, loss=0.37992829
val:105/157, loss=0.41278765
val:108/157, loss=0.36399451
val:111/157, loss=0.35337251
val:114/157, loss=0.40205970
val:117/157, loss=0.38106549
val:120/157, loss=0.35529912
val:123/157, loss=0.35953689
val:126/157, loss=0.38668758
val:129/157, loss=0.38904923
val:132/157, loss=0.36529237
val:135/157, loss=0.38303819
val:138/157, loss=0.39868790
val:141/157, loss=0.38165402
val:144/157, loss=0.36997491
val:147/157, loss=0.36459458
val:150/157, loss=0.37657875
val:153/157, loss=0.38903701
val:156/157, loss=0.37992561
loss=1.12405705
Train:000042/600000, Loss:2.42e-01, lr:1.00e-03, gradient:5.826242e-02
Train:000044/600000, Loss:2.37e-01, lr:1.00e-03, gradient:4.049075e-02
Train:000046/600000, Loss:2.31e-01, lr:1.00e-03, gradient:4.664306e-02
Train:000048/600000, Loss:2.37e-01, lr:1.00e-03, gradient:5.350446e-02
Train:000050/600000, Loss:2.47e-01, lr:1.00e-03, gradient:7.120595e-02
Elapsed time: 21.11s
============================================================
val:003/157, loss=0.35858780
val:006/157, loss=0.35295695
val:009/157, loss=0.36279154
val:012/157, loss=0.35020712
val:015/157, loss=0.33300221
val:018/157, loss=0.34775612
val:021/157, loss=0.33642465
val:024/157, loss=0.35931709
val:027/157, loss=0.38002238
val:030/157, loss=0.34976095
val:033/157, loss=0.35206592
val:036/157, loss=0.35197595
val:039/157, loss=0.34497017
val:042/157, loss=0.35323170
val:045/157, loss=0.32816237
val:048/157, loss=0.35572404
val:051/157, loss=0.34364283
val:054/157, loss=0.33719969
val:057/157, loss=0.36126375
val:060/157, loss=0.37677711
val:063/157, loss=0.36004591
val:066/157, loss=0.35097739
val:069/157, loss=0.34507865
val:072/157, loss=0.36450869
val:075/157, loss=0.32642585
val:078/157, loss=0.36057577
val:081/157, loss=0.35605085
val:084/157, loss=0.35545474
val:087/157, loss=0.36379012
val:090/157, loss=0.35529250
val:093/157, loss=0.35212350
val:096/157, loss=0.34217113
val:099/157, loss=0.36211634
val:102/157, loss=0.35577619
val:105/157, loss=0.38763627
val:108/157, loss=0.34027994
val:111/157, loss=0.32905877
val:114/157, loss=0.37642872
val:117/157, loss=0.35773951
val:120/157, loss=0.33206987
val:123/157, loss=0.33627874
val:126/157, loss=0.36094904
val:129/157, loss=0.36368749
val:132/157, loss=0.34053093
val:135/157, loss=0.35834387
val:138/157, loss=0.37448725
val:141/157, loss=0.35649601
val:144/157, loss=0.34377772
val:147/157, loss=0.34012204
val:150/157, loss=0.35060135
val:153/157, loss=0.36439025
val:156/157, loss=0.35465845
loss=1.05130637
Train:000052/600000, Loss:2.39e-01, lr:1.00e-03, gradient:1.551621e-02
Train:000054/600000, Loss:2.54e-01, lr:1.00e-03, gradient:1.757559e-02
Train:000056/600000, Loss:2.44e-01, lr:1.00e-03, gradient:7.369786e-02
Train:000058/600000, Loss:2.34e-01, lr:1.00e-03, gradient:5.419253e-02
Train:000060/600000, Loss:2.57e-01, lr:1.00e-03, gradient:7.053637e-02
Elapsed time: 21.64s
============================================================
val:003/157, loss=0.33941031
val:006/157, loss=0.33269054
val:009/157, loss=0.34558496
val:012/157, loss=0.33284032
val:015/157, loss=0.31447729
val:018/157, loss=0.32910496
val:021/157, loss=0.31727010
val:024/157, loss=0.33970797
val:027/157, loss=0.36110061
val:030/157, loss=0.33000851
val:033/157, loss=0.33343399
val:036/157, loss=0.33266586
val:039/157, loss=0.32430524
val:042/157, loss=0.33327240
val:045/157, loss=0.30995601
val:048/157, loss=0.33620346
val:051/157, loss=0.32334644
val:054/157, loss=0.31816307
val:057/157, loss=0.34129968
val:060/157, loss=0.35677123
val:063/157, loss=0.34132287
val:066/157, loss=0.33160755
val:069/157, loss=0.32712603
val:072/157, loss=0.34652597
val:075/157, loss=0.30694374
val:078/157, loss=0.34179801
val:081/157, loss=0.33698392
val:084/157, loss=0.33661258
val:087/157, loss=0.34257072
val:090/157, loss=0.33688384
val:093/157, loss=0.33444449
val:096/157, loss=0.32271838
val:099/157, loss=0.34341475
val:102/157, loss=0.33748323
val:105/157, loss=0.37053773
val:108/157, loss=0.32179290
val:111/157, loss=0.31150976
val:114/157, loss=0.35917541
val:117/157, loss=0.33744776
val:120/157, loss=0.31381369
val:123/157, loss=0.31690329
val:126/157, loss=0.34027696
val:129/157, loss=0.34454396
val:132/157, loss=0.32262504
val:135/157, loss=0.33911562
val:138/157, loss=0.35439831
val:141/157, loss=0.33656844
val:144/157, loss=0.32466048
val:147/157, loss=0.32069010
val:150/157, loss=0.33436859
val:153/157, loss=0.34515947
val:156/157, loss=0.33629280
loss=0.99459517
Train:000062/600000, Loss:2.46e-01, lr:1.00e-03, gradient:5.268429e-02
Train:000064/600000, Loss:2.48e-01, lr:1.00e-03, gradient:1.953888e-02
Train:000066/600000, Loss:2.65e-01, lr:1.00e-03, gradient:7.657245e-03
Train:000068/600000, Loss:2.33e-01, lr:1.00e-03, gradient:9.985783e-02
Train:000070/600000, Loss:2.58e-01, lr:1.00e-03, gradient:2.417528e-02
Elapsed time: 21.30s
============================================================
val:003/157, loss=0.32481855
val:006/157, loss=0.31810415
val:009/157, loss=0.32985240
val:012/157, loss=0.31620359
val:015/157, loss=0.29951411
val:018/157, loss=0.31435144
val:021/157, loss=0.30158430
val:024/157, loss=0.32445872
val:027/157, loss=0.34626684
val:030/157, loss=0.31544679
val:033/157, loss=0.31869042
val:036/157, loss=0.31793916
val:039/157, loss=0.31014231
val:042/157, loss=0.31895328
val:045/157, loss=0.29232231
val:048/157, loss=0.32200104
val:051/157, loss=0.30942771
val:054/157, loss=0.30169645
val:057/157, loss=0.32622331
val:060/157, loss=0.34159178
val:063/157, loss=0.32571363
val:066/157, loss=0.31673020
val:069/157, loss=0.31298241
val:072/157, loss=0.33146250
val:075/157, loss=0.29276910
val:078/157, loss=0.32749724
val:081/157, loss=0.32144076
val:084/157, loss=0.32204172
val:087/157, loss=0.32921416
val:090/157, loss=0.32179308
val:093/157, loss=0.31823805
val:096/157, loss=0.30757099
val:099/157, loss=0.32773414
val:102/157, loss=0.32185498
val:105/157, loss=0.35475004
val:108/157, loss=0.30717933
val:111/157, loss=0.29692054
val:114/157, loss=0.34247077
val:117/157, loss=0.32250533
val:120/157, loss=0.29742002
val:123/157, loss=0.30142528
val:126/157, loss=0.32646278
val:129/157, loss=0.33048227
val:132/157, loss=0.30634868
val:135/157, loss=0.32508415
val:138/157, loss=0.34103507
val:141/157, loss=0.32305425
val:144/157, loss=0.31054097
val:147/157, loss=0.30613551
val:150/157, loss=0.31807208
val:153/157, loss=0.33002159
val:156/157, loss=0.32053515
loss=0.94919157
Train:000072/600000, Loss:2.49e-01, lr:1.00e-03, gradient:3.052292e-02
Train:000074/600000, Loss:2.59e-01, lr:1.00e-03, gradient:4.936829e-02
Train:000076/600000, Loss:2.35e-01, lr:1.00e-03, gradient:4.832117e-02
Train:000078/600000, Loss:2.38e-01, lr:1.00e-03, gradient:4.834294e-02
Train:000080/600000, Loss:2.56e-01, lr:1.00e-03, gradient:3.854532e-02
Elapsed time: 21.01s
============================================================
val:003/157, loss=0.31112009
val:006/157, loss=0.30666459
val:009/157, loss=0.31715113
val:012/157, loss=0.30581367
val:015/157, loss=0.28651640
val:018/157, loss=0.30227453
val:021/157, loss=0.28964937
val:024/157, loss=0.31378928
val:027/157, loss=0.33379841
val:030/157, loss=0.30311143
val:033/157, loss=0.30555403
val:036/157, loss=0.30643097
val:039/157, loss=0.29797715
val:042/157, loss=0.30791977
val:045/157, loss=0.28154203
val:048/157, loss=0.30895200
val:051/157, loss=0.29697037
val:054/157, loss=0.28907925
val:057/157, loss=0.31430939
val:060/157, loss=0.33010584
val:063/157, loss=0.31460822
val:066/157, loss=0.30424058
val:069/157, loss=0.30011258
val:072/157, loss=0.32007921
val:075/157, loss=0.27972913
val:078/157, loss=0.31367391
val:081/157, loss=0.30884355
val:084/157, loss=0.30944571
val:087/157, loss=0.31622228
val:090/157, loss=0.30940491
val:093/157, loss=0.30530843
val:096/157, loss=0.29665685
val:099/157, loss=0.31610009
val:102/157, loss=0.30978853
val:105/157, loss=0.34287542
val:108/157, loss=0.29424581
Traceback (most recent call last):
  File "main.py", line 32, in <module>
    trainer.train()
  File "/data/FFHQ/DifFace_Thesis/trainer.py", line 323, in train
    self.validation()
  File "/data/FFHQ/DifFace_Thesis/trainer.py", line 1298, in validation
    self.logging_image(data['mask'], tag="mask", phase=phase, add_global_step=False)
  File "/data/FFHQ/DifFace_Thesis/trainer.py", line 369, in logging_image
    im_tensor = vutils.make_grid(im_tensor, nrow=nrow, normalize=True, scale_each=True) # c x H x W
  File "/usr/local/anaconda3/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/usr/local/anaconda3/lib/python3.8/site-packages/torchvision/utils.py", line 96, in make_grid
    norm_range(t, value_range)
  File "/usr/local/anaconda3/lib/python3.8/site-packages/torchvision/utils.py", line 92, in norm_range
    norm_ip(t, float(t.min()), float(t.max()))
  File "/usr/local/anaconda3/lib/python3.8/site-packages/torchvision/utils.py", line 86, in norm_ip
    img.sub_(low).div_(max(high - low, 1e-5))
KeyboardInterrupt
Traceback (most recent call last):
  File "main.py", line 32, in <module>
    trainer.train()
  File "/data/FFHQ/DifFace_Thesis/trainer.py", line 323, in train
    self.validation()
  File "/data/FFHQ/DifFace_Thesis/trainer.py", line 1298, in validation
    self.logging_image(data['mask'], tag="mask", phase=phase, add_global_step=False)
  File "/data/FFHQ/DifFace_Thesis/trainer.py", line 369, in logging_image
    im_tensor = vutils.make_grid(im_tensor, nrow=nrow, normalize=True, scale_each=True) # c x H x W
  File "/usr/local/anaconda3/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/usr/local/anaconda3/lib/python3.8/site-packages/torchvision/utils.py", line 96, in make_grid
    norm_range(t, value_range)
  File "/usr/local/anaconda3/lib/python3.8/site-packages/torchvision/utils.py", line 92, in norm_range
    norm_ip(t, float(t.min()), float(t.max()))
  File "/usr/local/anaconda3/lib/python3.8/site-packages/torchvision/utils.py", line 86, in norm_ip
    img.sub_(low).div_(max(high - low, 1e-5))
KeyboardInterrupt
